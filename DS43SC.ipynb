{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS43SC.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Y6SKlgYrpcym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BrEbRrjVphPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron\n",
        "- Input Layer\n",
        "- Hidden Layer\n",
        "- Output Layer\n",
        "- Activation\n",
        "- Backpropagation"
      ]
    },
    {
      "metadata": {
        "id": "Q5EksLqnp4oB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Neuron: A Neuron is a brain / nerve cell that serves as a logic gate in our Brain. An Artificial Neuron is a mathematical function model of the cell. It calculates some equations on the input and decides to return a 1 or 0. In similar fashion a brain neuron receives multiple stimuli and then triggers the sending of a signal when appropriate (and sometimes when not appropriate).\n",
        "\n",
        "* Input layer: The input layer is the input variables, sometimes called the \"visable layer\".\n",
        "\n",
        "* Hidden Layer: The one or more processing layers of varying sizes and configurations that make up the MLP or deep learning model. Hidden layers are required if and only if the data must be separated non-linearly. By adding additional layers we can handle increasingly complex data relationships .\n",
        "\n",
        "* Output Layer: The collection of output nodes that are responsible for computations and transferring information from the network to the outside world. These evaluate the combinations from the weights and hidden layer outputs and assign a final score. \n",
        "\n",
        "* Activation: Activation functions are used to determine how an input signal is pushed forward by a node. It does this by applying the activation function which maps the input value against an established thresholding function that constrains the output within the desired range (usually 0, 1 or -1, 1). By doing so each node can then produce a constrained output.\n",
        "\n",
        "* Backpropagation:  In backpropagation we take the error (the \"cost\" that we compute by comparing the calculated output and the known, correct target output) and we then use it to update the model parameters. We calculate the total error at the output nodes and propagate these errors back through the network in each layer. This causes our network to perform better when calculating gradients than before since the weights have now been adjusted to minimize the error in prediction."
      ]
    },
    {
      "metadata": {
        "id": "Ri_gRA2Jp728",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "metadata": {
        "id": "Ig6ZTH8tpQ19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "60ba6c08-87d7-44b8-be18-5ec2bc2364bb"
      },
      "cell_type": "code",
      "source": [
        "# Start by creating weights and bias\n",
        "weight1 = 0.3334\n",
        "weight2 = 0.3334\n",
        "weight3 = 0.3334\n",
        "bias = -1\n",
        "\n",
        "\n",
        "# Add inputs and outputs\n",
        "test_inputs = [(1, 1, 1), (1, 0, 1), (0, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 0), (0,0,0)]\n",
        "correct_outputs = [True, False, False, False]\n",
        "outputs = []\n",
        "\n",
        "# Generate and check output\n",
        "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
        "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + weight3 * test_input[2] + bias\n",
        "    output = int(linear_combination >= 0)\n",
        "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
        "    outputs.append([test_input[0], test_input[1],  test_input[2], linear_combination, output, is_correct_string])\n",
        "\n",
        "# Print output\n",
        "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', 'Input 3','  Linear Combination', '  Activation Output', '  Is Correct'])\n",
        "print(output_frame.to_string(index=False))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input 1    Input 2  Input 3    Linear Combination    Activation Output   Is Correct\n",
            "      1          1        1                0.0002                    1          Yes\n",
            "      1          0        1               -0.3332                    0          Yes\n",
            "      0          1        1               -0.3332                    0          Yes\n",
            "      0          0        1               -0.6666                    0          Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "86HyRi8Osr3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "metadata": {
        "id": "CNfiajv3v4Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a401dccd-c94b-4270-ab26-64cf7d14e783"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv\"\n",
        "\n",
        "df = pd.read_csv(data, header=0)\n",
        "display(df.shape)\n",
        "df = df.dropna(axis=0, how='any')\n",
        "display(df.shape)\n",
        "display(df.head(5))\n",
        "display(df.target.value_counts())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(303, 14)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(303, 14)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1    165\n",
              "0    138\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "cjBbnvecRw05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetMLP(object):\n",
        "    \"\"\"\n",
        "    Feed forward nueral network / multi-layer perceptron classifier\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_hidden : int (default: 30)\n",
        "        Number of hidden units\n",
        "    l2 : float (default 0.)\n",
        "        Lambda value for l2 normalization\n",
        "    epochs : int (default: 100)\n",
        "        Number of training epochs\n",
        "    eta : float (default = 0.001)\n",
        "        Learning rate\n",
        "    shuffle : bool (default: True)\n",
        "        Shuffles the training data every epoch if True\n",
        "    minibatch_size : int (default : 1)\n",
        "        Number of training samples per minibatch\n",
        "    seed : int (default: None)\n",
        "        Random seed for initalizing weights and shuffling\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    eval_ : dict\n",
        "        Dictionary collecting the cost, training accuracy,\n",
        "        and validation accuracy for each epoch during training\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_hidden=30, l2=0.,\n",
        "                epochs=100, eta=0.001,\n",
        "                shuffle=True, minibatch_size=1, seed=None):\n",
        "        self.random = np.random.RandomState(seed)\n",
        "        self.n_hidden = n_hidden\n",
        "        self.l2 = l2\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.shuffle = shuffle\n",
        "        self.minibatch_size = minibatch_size\n",
        "        \n",
        "    def _onehot(self, y, n_classes):\n",
        "        \"\"\"\n",
        "        Encode labels into one hot representation\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : array, shape = [n_samples]\n",
        "            Target values\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        onehot : array, shape = (n_samples, n_labels)\n",
        "        \"\"\"\n",
        "        \n",
        "        onehot = np.zeros((n_classes, y.shape[0]))\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "            onehot[val, idx] = 1.\n",
        "        return onehot.T\n",
        "    \n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Compute logistic function (sigmoid)\n",
        "        \"\"\"\n",
        "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
        "    \n",
        "    def _forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute forward propogation step\n",
        "        \"\"\"\n",
        "        # step 1: net input of hidden layer\n",
        "        z_h = np.dot(X, self.w_h) + self.b_h\n",
        "        \n",
        "        # step 2: activation of hidden layer\n",
        "        a_h = self._sigmoid(z_h)\n",
        "        \n",
        "        # step 3: net input of output layer\n",
        "        z_out = np.dot(a_h, self.w_out) + self.b_out\n",
        "        \n",
        "        # step 4: activation layer output\n",
        "        a_out = self._sigmoid(z_out)\n",
        "        \n",
        "        return z_h, a_h, z_out, a_out\n",
        "    \n",
        "    def _compute_cost(self, y_enc, output):\n",
        "        \"\"\"\n",
        "        Compute cost function\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_enc : array, shape = (n_samples, n_labels)\n",
        "            one-hot encoded class labels\n",
        "        output : array, shape = [n_samples, n_output_units]\n",
        "            Activation of the output layer (forward propoagation)\n",
        "        Returns\n",
        "        -------\n",
        "        cost : float\n",
        "            Regularized cost\n",
        "        \"\"\"\n",
        "        e = 0.000000001\n",
        "        L2_term = (self.l2 *\n",
        "                  (np.sum(self.w_h ** 2.) + \n",
        "                  np.sum(self.w_out ** 2.)))\n",
        "        term1 = -y_enc * (np.log(output + e))\n",
        "        term2 = (1. - y_enc) * np.log(1. - output + e)\n",
        "        cost = np.sum(term1 - term2) + L2_term\n",
        "        return cost\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels\n",
        "        \n",
        "        Paramters\n",
        "        ---------\n",
        "        X : array, shape = [n_samples, n_features]\n",
        "            Input layer with original features\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : array, shape = [n_samples]\n",
        "            Predicted class labels\n",
        "        \"\"\"\n",
        "        z_h, a_h, z_out, a_out = self._forward(X)\n",
        "        y_pred = np.argmax(z_out, axis=1)\n",
        "        return y_pred\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
        "        \"\"\"\n",
        "        Learn weights from training data\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X_train : array, shape = [n_samples, n_features]\n",
        "            Input layer with original features\n",
        "        y_train : array, shape = [n_samples]\n",
        "            Target class labels\n",
        "        X_test : array, shape = [n_samples, n_features]\n",
        "            Sample features for validation during training\n",
        "        y_test : array, shape = [n_samples]\n",
        "            Samples test class labels for validation during training\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        n_output = np.unique(y_train).shape[0]\n",
        "        n_features = X_train.shape[1]\n",
        "        \n",
        "        #######################\n",
        "        # Weight Initialization\n",
        "        #######################\n",
        "        \n",
        "        # weights for input -> hidden\n",
        "        self.b_h = np.zeros(self.n_hidden)\n",
        "        self.w_h = self.random.normal(loc=0.0,\n",
        "                                     scale=0.1,\n",
        "                                     size=(n_features,\n",
        "                                          self.n_hidden))\n",
        "        \n",
        "        # weights for hidden -> output\n",
        "        self.b_out = np.zeros(n_output)\n",
        "        self.w_out = self.random.normal(loc=0.0,\n",
        "                                       scale=0.1,\n",
        "                                       size=(self.n_hidden,\n",
        "                                       n_output))\n",
        "        \n",
        "        epoch_strlen = len(str(self.epochs)) # for progr. format\n",
        "        self.eval_ = {'cost' : [],\n",
        "                     'train_acc' : [],\n",
        "                     'valid_acc' : []}\n",
        "        \n",
        "        y_train_enc = self._onehot(y_train, n_output)\n",
        "        \n",
        "        # iterate over training epochs\n",
        "        for i in range(self.epochs):\n",
        "            \n",
        "            # iterate over mini batches\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "            \n",
        "            if self.shuffle:\n",
        "                self.random.shuffle(indices)\n",
        "            \n",
        "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
        "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "                \n",
        "                # forward propagation\n",
        "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
        "                \n",
        "                #################\n",
        "                # Backpropagation\n",
        "                #################\n",
        "                \n",
        "                # [n_samples, n_classlabels]\n",
        "                sigma_out = a_out - y_train_enc[batch_idx]\n",
        "                \n",
        "                # [n_samples, n_hidden]\n",
        "                sigmoid_derivative_h = a_h * (1. - a_h)\n",
        "                \n",
        "                # [n_samples, n_classlabels] dot [n_classlabels, n_hidden]\n",
        "                # -> [n_samples, n_hidden]\n",
        "                sigma_h = (np.dot(sigma_out, self.w_out.T) * sigmoid_derivative_h)\n",
        "                \n",
        "                # [n_features, n_samples] dot [n_samples, n_hidden]\n",
        "                # -> [n_features, n_hidden]\n",
        "                grad_w_h = np.dot(X_train[batch_idx].T, sigma_h)\n",
        "                grad_b_h = np.sum(sigma_h, axis=0)\n",
        "                \n",
        "                # [n_hidden, n_samples] dot [n_samples, n_classlabels]\n",
        "                # -> [n_hidden, n_classlabels]\n",
        "                grad_w_out = np.dot(a_h.T, sigma_out)\n",
        "                grad_b_out = np.sum(sigma_out, axis=0)\n",
        "                \n",
        "                # Regularization and weight updates\n",
        "                delta_w_h = (grad_w_h + self.l2 * self.w_h)\n",
        "                delta_b_h = grad_b_h # bias is not regularized\n",
        "                self.w_h -= self.eta * delta_w_h\n",
        "                self.b_h -= self.eta * delta_b_h\n",
        "                \n",
        "                delta_w_out = grad_w_out + self.l2 * self.w_out\n",
        "                delta_b_out = grad_b_out # bias is not regularized\n",
        "                self.w_out -= self.eta * delta_w_out\n",
        "                self.b_out -= self.eta * delta_b_out\n",
        "                \n",
        "            ############\n",
        "            # Evaluation\n",
        "            ############\n",
        "\n",
        "            # evaluation after each epoch during training\n",
        "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
        "            \n",
        "            cost = self._compute_cost(y_enc=y_train_enc, output=a_out)\n",
        "            \n",
        "            y_train_pred = self.predict(X_train)\n",
        "            y_valid_pred = self.predict(X_valid)\n",
        "            \n",
        "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) / X_train.shape[0])\n",
        "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) / X_valid.shape[0])\n",
        "            \n",
        "            sys.stderr.write('\\r%0*d/%d | Cost: %.2f | Train/Valid Acc: %.2f%%/%.2f%%'\n",
        "                            % (epoch_strlen, i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
        "            sys.stderr.flush()\n",
        "            \n",
        "            self.eval_['cost'].append(cost)\n",
        "            self.eval_['train_acc'].append(train_acc)\n",
        "            self.eval_['valid_acc'].append(valid_acc)\n",
        "            \n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "815E5qeQSnh1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize a new MLP\n",
        "mlp = NeuralNetMLP(n_hidden= 26,\n",
        "                   l2= 0.01,\n",
        "                   epochs= 200,\n",
        "                   eta= 0.0005,\n",
        "                   minibatch_size= 100,\n",
        "                   shuffle = True,\n",
        "                   seed = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RyUiRQZUVOAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ee7c9038-f9ed-4a90-ffad-b0596eca8a28"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X = df.drop(columns = \"target\")\n",
        "X = MinMaxScaler().fit_transform(X)\n",
        "y = df.target.copy()\n",
        "\n",
        "display(X.shape)\n",
        "display(y.shape)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(303, 13)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(303,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GGT1oRzXw3H9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "metadata": {
        "id": "XWw4IYxLxKwH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##### Your Code Here #####"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}